{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in2=\"/Users/agne/Desktop/dt/wbl_only.tsv\"\n",
    "counts_gtex_wbl_pd = pd.read_csv(path_in2, index_col=0,header=0, sep=\"\\t\")\n",
    "counts_gtex_wbl = np.transpose(np.array(counts_gtex_wbl_pd.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autoCorrect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1a4baa566c1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautoCorrect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrectors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAECorrector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autoCorrect'"
     ]
    }
   ],
   "source": [
    "from autoCorrect.correctors import AECorrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data!\n",
      "Using OutInjectionFC method!\n",
      "Injecting!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 't6' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-487b9a0f97b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAECorrector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_gtex_wbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_gtex_wbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-2333517c3575>\u001b[0m in \u001b[0;36mcorrect\u001b[0;34m(self, counts, size_factors, only_predict)\u001b[0m\n\u001b[1;32m     58\u001b[0m                                   \u001b[0mencoding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                                   size=counts.shape[1])\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             self.ae.model.fit(self.data[0][0], self.data[0][1],  \n\u001b[1;32m     62\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/Keras-2.0.8-py3.6.egg/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 850\u001b[0;31m                                                 sample_weight, mask)\n\u001b[0m\u001b[1;32m    851\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/Keras-2.0.8-py3.6.egg/keras/engine/training.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    448\u001b[0m         \"\"\"\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workdir/autoCorrect/src/autoCorrect/loss.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, y_true, y_pred, mean)\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_tensor_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m't4 has inf/nans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_tensor_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m't5 has inf/nans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_tensor_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m't6 has inf/nans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                     \u001b[0;31m#tf.verify_tensor_all_finite(t7, 't7 has inf/nans')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 't6' is not defined"
     ]
    }
   ],
   "source": [
    "co = AECorrector().correct(counts_gtex_wbl, np.ones_like(counts_gtex_wbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.mean(counts_gtex_wbl,axis=0)==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271, 18259)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_gtex_wbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last loss:  7.83128521449\n",
      "Last validation loss:  8.5203119278\n"
     ]
    }
   ],
   "source": [
    "co =correctors.DAECorrector(counts_gtex_wbl, np.ones_like(counts_gtex_wbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.21866241e+02,   1.85743362e+02,   1.01053770e+04, ...,\n",
       "          9.85023047e+03,   5.38190556e+00,   4.74372578e+04],\n",
       "       [  2.97523895e+02,   7.08913651e+01,   4.87940967e+03, ...,\n",
       "          1.09737666e+04,   1.20186911e+01,   3.33794766e+04],\n",
       "       [  2.76929382e+02,   1.13092812e+02,   5.82256836e+03, ...,\n",
       "          1.95924121e+04,   5.59521532e+00,   5.22704648e+04],\n",
       "       ..., \n",
       "       [  2.46393250e+02,   6.52215500e+01,   4.75787939e+03, ...,\n",
       "          6.80952197e+03,   6.25102234e+00,   2.96955000e+04],\n",
       "       [  5.36501587e+02,   3.12589817e+01,   7.33467334e+03, ...,\n",
       "          2.65664551e+04,   8.20590115e+00,   2.31300906e+05],\n",
       "       [  3.68015289e+02,   4.93917580e+01,   4.57248584e+03, ...,\n",
       "          1.36107832e+04,   5.39488316e+00,   6.81139062e+04]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Multiply\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from loss import NB\n",
    "from layers import ConstantDispersionLayer\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as mp #TODO do we need matplotlib?\n",
    "from keras.optimizers import Adam, RMSprop #TODO do we need both optimizers?\n",
    "import matplotlib.pyplot as plt #TODO do we need matplotlib?\n",
    "#from .myCallbacks import PlotLosses # TODO what is this?\n",
    "from keras import losses\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import h5py \n",
    "\n",
    "#TODO cleanup\n",
    "\n",
    "class Autoencoder():\n",
    "    def __init__(self, train_in=None, sf_train=None, test_in=None,\n",
    "                 sf_test=None, train_out=None, test_out=None,\n",
    "                 predict_data=None, sf_predict=None, means_data=None, \n",
    "                 encoding_dim=2, size=10000, optimizer=Adam(lr=0.001),\n",
    "                 choose_autoencoder=False, choose_encoder=False,\n",
    "                 choose_decoder=False, epochs=1100, batch_size=None):\n",
    "        self.train_in = train_in\n",
    "        self.sf_train = sf_train\n",
    "        self.test_in = test_in\n",
    "        self.sf_test = sf_test\n",
    "        self.train_out = train_out\n",
    "        self.test_out = test_out\n",
    "        self.predict_data = predict_data\n",
    "        self.sf_predict = sf_predict\n",
    "        self.means_data = means_data\n",
    "        self.val_losses = []\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.mean_cutoff = 0.01\n",
    "        self.size = size\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.ClippedExp = lambda x: K.minimum(K.exp(x), 1e5)\n",
    "        self.Mean_cutoff = lambda x: K.maximum(x, self.mean_cutoff)\n",
    "        self.Invert = lambda x: K.pow(x, -1) \n",
    "        self.Loglayer = lambda x: K.log(x + 1)\n",
    "        self.choose_autoencoder = choose_autoencoder\n",
    "        self.choose_encoder = choose_encoder\n",
    "        self.choose_decoder = choose_decoder\n",
    "        self.autoenc_model = self.get_autoencoder()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = self.set_loss()\n",
    "        self.model = self.set_model()\n",
    "        self.sess = tf.Session()\n",
    "        K.set_session(self.sess)\n",
    "        self.run_session()\n",
    "\n",
    "\n",
    "\n",
    "    def get_autoencoder(self, encoding_dim=None):\n",
    "        if encoding_dim != None:\n",
    "            self.encoding_dim = encoding_dim\n",
    "        self.input_layer = Input(shape=(self.size,), name='inp')\n",
    "        self.sf_layer = Input(shape=(self.size,), name='sf')\n",
    "        self.normalized = Multiply()([self.input_layer, self.sf_layer]) #size factor layer contains inversed sf see : data_utils.py \n",
    "        self.logcounts = Lambda(self.Loglayer, output_shape=(self.size,), name=\"logCounts\")(self.normalized)\n",
    "        encoded = Dense(self.encoding_dim, name='encoder', use_bias=True)(self.logcounts)\n",
    "        decoded = Dense(self.size, name='decoder', use_bias=True)(encoded)\n",
    "        mean_scaled = Lambda(self.ClippedExp, output_shape=(self.size,), name=\"mean_scaled\")(decoded)\n",
    "        inv_sf = Lambda(self.Invert, output_shape=(self.size,), name=\"inv\")(self.sf_layer)\n",
    "        mean = Multiply()([mean_scaled, inv_sf])\n",
    "        mean_min = Lambda(self.Mean_cutoff, output_shape=(self.size,), name=\"mean_min\")(mean)\n",
    "        self.disp = ConstantDispersionLayer(name='dispersion')\n",
    "        self.output = self.disp(mean_min)\n",
    "        self.model = Model([self.input_layer, self.sf_layer], self.output)\n",
    "        return self.model\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        self.encoder = Model([self.autoenc_model.get_layer('inp').input, \n",
    "                              self.autoenc_model.get_layer('sf').input],\n",
    "                             self.autoenc_model.get_layer('encoder').output)\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        encoded_input = Input(shape=(self.encoding_dim,))\n",
    "        decoder_layer = self.autoenc_model.get_layer('decoder')\n",
    "        decoded = decoder_layer(encoded_input)\n",
    "        mean_layer = self.autoenc_model.get_layer('mean')\n",
    "        mean = mean_layer(decoded)\n",
    "        dispersion_layer = ConstantDispersionLayer(name='dispersion')\n",
    "        output = dispersion_layer(mean)\n",
    "        self.decoder = Model(encoded_input, output)\n",
    "        return self.decoder\n",
    "\n",
    "    def set_loss(self):\n",
    "        if self.choose_autoencoder:\n",
    "            nb = NB(self.model.get_layer('dispersion').theta)\n",
    "            self.loss = nb.loss\n",
    "        elif self.choose_encoder:\n",
    "            self.loss = losses.mean_squared_error\n",
    "        elif self.choose_decoder:\n",
    "            nb = NB(self.model.get_layer('dispersion').theta)\n",
    "            self.loss = nb.loss\n",
    "        return self.loss\n",
    "\n",
    "    def set_model(self):\n",
    "        if self.choose_autoencoder:\n",
    "            self.model = self.get_autoencoder()\n",
    "        elif self.choose_encoder:\n",
    "            self.model = self.get_encoder()\n",
    "        elif self.choose_decoder:\n",
    "            self.model = self.get_decoder()\n",
    "        return self.model\n",
    "    \n",
    "    def compile_model(self):\n",
    "        self.model.compile(optimizer=self.optimizer, loss=self.loss)\n",
    "        \n",
    "\n",
    "    def run_session(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def fit_model(self):\n",
    "        with self.sess.as_default():\n",
    "            self.history = self.model.fit({'inp':self.train_in, 'sf':self.sf_train}, self.train_out,  \n",
    "                                          epochs=self.epochs,\n",
    "                                          batch_size=self.batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          validation_data=({'inp':self.test_in,'sf':self.sf_test}, self.test_out), \n",
    "                                          verbose=0\n",
    "                                          )\n",
    "\n",
    "    def get_current_val_loss(self):\n",
    "        with self.sess.as_default():\n",
    "            self.val_loss = self.loss(self.test_in, self.test_out).eval()\n",
    "        return self.val_loss\n",
    "    \n",
    "    def get_pred_val_loss(self):\n",
    "        with self.sess.as_default():\n",
    "            self.val_loss = self.loss(self.pred_data, self.pred_data).eval()\n",
    "        return self.val_loss\n",
    "\n",
    "    def _predict_test(self):\n",
    "        self.predicted_test = self.model.predict([self.test_in,self.sf_test])\n",
    "        return self.predicted_test\n",
    "    \n",
    "    def _predict_train(self):\n",
    "        self.predicted_train = self.model.predict([self.train_in,self.sf_train])\n",
    "        return self.predicted_train\n",
    "    \n",
    "    def predict(self):\n",
    "        self.predicted = self.model.predict([self.predict_data, self.sf_predict])\n",
    "        return self.predicted\n",
    "\n",
    "    def get_dispersion(self):\n",
    "        with self.sess.as_default():\n",
    "            self.predicted_dispersion = self.model.get_layer('dispersion').theta.eval()\n",
    "        return self.predicted_dispersion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from data_utils import DataCooker\n",
    "from layers import ConstantDispersionLayer\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "class Corrector():        \n",
    "    @abstractmethod\n",
    "    def correct(self, counts, size_factors, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DummyCorrector(Corrector):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def correct(self, counts, size_factors, **kwargs):\n",
    "        return np.ones_like(self.counts)\n",
    "\n",
    "\n",
    "class AECorrector(Corrector):\n",
    "    def __init__(self, model_name=\"model\", model_directory=\"/s/project/scared/model\",\n",
    "                 param_path=None, denoisingAE=True,\n",
    "                 save_model=True, epochs=250, encoding_dim=23, lr=0.00068, batch_size=None):\n",
    "        if param_path is not None:\n",
    "            metrics = json.load(open(param_path))\n",
    "            self.denoisingAE = metrics['inj_out'] \n",
    "            self.epochs = metrics['epochs']\n",
    "            self.encoding_dim = metrics['m_emb'] \n",
    "            self.lr = metrics['m_lr'] \n",
    "        else:\n",
    "            self.denoisingAE = denoisingAE\n",
    "            self.epochs = epochs\n",
    "            self.encoding_dim = encoding_dim\n",
    "            self.lr = lr\n",
    "            self.batch_size = batch_size\n",
    "            self.save_model = save_model\n",
    "            self.model_name = model_name\n",
    "            self.directory = model_directory\n",
    "\n",
    "    def correct(self, counts, size_factors, only_predict=False):\n",
    "        if len(counts.shape) == 1:\n",
    "            counts = counts.reshape(1,counts.shape[0])\n",
    "            size_factors = size_factors.reshape(1,size_factors.shape[0])\n",
    "        if counts.shape != size_factors.shape:\n",
    "            raise ValueError(\"Size factors and counts must have equal dimensions.\"+\n",
    "                             \"\\nNow counts shape:\"+str(counts.shape)+ \\\n",
    "                            \"\\nSize factors shape:\"+str(size_factors.shape))\n",
    "        self.loader = DataCooker(counts, size_factors,\n",
    "                                 inject_outliers=self.denoisingAE,\n",
    "                                 only_prediction=only_predict)\n",
    "        self.data = self.loader.data()\n",
    "        if not only_predict:\n",
    "            self.ae = Autoencoder(choose_autoencoder=True,\n",
    "                                  encoding_dim=self.encoding_dim,\n",
    "                                  size=counts.shape[1])\n",
    "            self.ae.model.compile(optimizer=Adam(lr=self.lr), loss=self.ae.loss)\n",
    "            self.ae.model.fit(self.data[0][0], self.data[0][1],  \n",
    "                                epochs=self.epochs,\n",
    "                                batch_size=self.batch_size,\n",
    "                                shuffle=True,\n",
    "                                validation_data=(self.data[1][0], self.data[1][1]),\n",
    "                                verbose=1\n",
    "                               )\n",
    "            if self.save_model:\n",
    "                model_json = self.ae.model.to_json()\n",
    "                with open(self.directory+'/'+self.model_name+'.json', \"w\") as json_file: \n",
    "                    json_file.write(model_json)\n",
    "                self.ae.model.save_weights(self.directory+'/'+self.model_name+'_weights.h5')\n",
    "                print(\"Model saved on disk!\")\n",
    "            model = self.ae.model\n",
    "        else:\n",
    "            json_file = open(self.directory+'/'+self.model_name+'.json', 'r') \n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "            model = model_from_json(loaded_model_json,\n",
    "                                            custom_objects={'ConstantDispersionLayer': ConstantDispersionLayer})\n",
    "            model.load_weights(self.directory+'/'+self.model_name+'_weights.h5')\n",
    "            print(\"Model loaded from disk!\")\n",
    "        self.corrected = model.predict(self.data[2][0])\n",
    "        return self.corrected\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
